+++
title = "Luca Soldaini"
+++

<div id="avatar-container">
    <div class="avatar-glow" aria-hidden="true">
        <div class="avatar-blob avatar-blob-1"></div>
        <div class="avatar-blob avatar-blob-2"></div>
        <div class="avatar-blob avatar-blob-3"></div>
        <div class="avatar-blob avatar-blob-4"></div>
        <div class="avatar-blob avatar-blob-5"></div>
    </div>
    <div id="front-avatar">
        <img src="personal-me/me-512.webp" alt="Luca Soldaini" class="avatar">
    </div>
    <div id="back-avatar">
        <img src="/alt.webp" loading="lazy" alt="A raccoon wearing a top hat and holding a pizza slice. Luca sometimes uses this image as their online profile picture." class="avatar">
        <p class="tiny-text center caption-avatar">DALLâ€¢E 2 generation (April 2022)</p>
    </div>
</div>

<p>
Hello, visitor!<span class="icon-wave" aria-hidden="true"></span>
</p>

<div id='about-me'>

I am a **lead research scientist** at [Ai2][6] in the [Olmo][35] team.
Prior to joining Ai2, I was a senior applied scientist at [Amazon Alexa][1].
I completed my Ph.D. in computer science at [Georgetown University][4] in 2018 in the [Information Retrieval Lab][34] working with [Nazli Goharian][33].

When not in front of a screen, I enjoy [brewing espresso][9], going on [runs][8], dreaming about utopian [mass transit systems][10], curating my ever-growing [laptop stickers collection][11], and hanging out with my [handsome][44] [cats][45]. Raccoons are [the best][13].

</div>
<div id='research-summary'>

## Research Interests

These days, my research focuses on maximizing transparency in all aspects of how large language models (LLMs) are created, trained, and evaluated.

- I co-lead the **data team for [Olmo][35]**, Ai2's language model project with [Kyle Lo][43]. Olmo is a state-of-the-art, fully-open model designed to accelerate the science of LLMs. In the last 3 years, we released [dense][38] and [mixture-of-experts][37] variants, alongside the data, code, recipes, and checkpoints we used to build them. Our latest is [Olmo 3][46]: we release instruct and thinking 7B/32B models that are about competitive with Qwen 3.
- With my colleagues at Ai2, I develop **recipes for adaptation** of language models. In 2024, we launched [Tulu 3][39], a state-of-the-art pipeline to post-train language models [up to 405B parameters][40]. I worked on [Molmo][41], a family of open multimodal AI models that are as good as closed-source VLMs. We recently released [DR Tulu][47], a fully-open recipe for building deep research systems; [better than][48] Gemini 3 Pro and GPT-5!
- collaborated on several **toolkits** for pretraining **data curation**. [olmOCR](https://olmocr.allenai.org) is a high-performance model for PDF text extraction; [v2](https://arxiv.org/abs/2510.19817) is even better thanks to RL on verifiable unit tests! [WebOrganizer](https://weborganizer.allen.ai) can partition large-scale web data by topic and format.

Hop over to the [publications page](/publications) for a complete list of my work.
</div>

<div id='contacts'>

## Contacts

I am mostly active on Twitter/X, and host most of my code on GitHub. The best way to reach out is via email; my Signal contact is available upon request.

<div id="contact-list">
<ul class="fa-ul contact-list">
    <li class="contact-item">
        <span class="list-icon icon-twitter" aria-hidden="true"></span>
        <a href="https://twitter.com/soldni">Twitter / X</a>
    </li>
    <li class="contact-item">
        <span class="list-icon icon-linkedin" aria-hidden="true"></span>
        <a href="https://www.linkedin.com/in/soldni" target="_blank">LinkedIn</a>
    </li>
    <li class="contact-item">
        <span class="list-icon icon-github" aria-hidden="true"></span>
        <a href="https://github.com/soldni" target="_blank">GitHub</a>
    </li>
    <li class="contact-item">
        <span class="list-icon icon-gs" aria-hidden="true"></span>
        <a href="https://scholar.google.com/citations?user=3KPvwcgAAAAJ" target="_blank">Google Scholar</a>
    </li>
    <li class="contact-item">
        <span class="list-icon icon-email" aria-hidden="true"></span>
        <a href="mailto:luca@soldaini.net">Email</a>
    </li>
</ul>
</div>

[1]: https://www.amazon.science/search?q=Luca+Soldaini&type=91d74bfc-4a20-30f0-8926-e52f02f15c04&type=5be10472-b2e0-37b5-b6f8-8f381832e94f&type=4f8e492c-6f2f-390e-bc61-f176d3a37ab9&s=0&expandedFilters=Type%2CResearch%2520area%2CTag%2CConference%2CJournal%2CAuthor%2CDate%2C
[2]: https://www.google.com/maps/place/Manhattan+Beach,+CA+90266/
[3]: https://www.ing-inl.unifi.it
[4]: https://cs.georgetown.edu/
[5]: http://queerinai.org/
[6]: https://allenai.org
[7]: https://research.semanticscholar.org
[8]: /marathon.webp
[9]: /espresso.webp
[10]: /transit.webp
[11]: /laptop.webp
[12]: https://twitter.com/soldni/status/1444411540480749569
[13]: https://www.youtube.com/watch?v=Ofp26_oc4CA
[14]: http://hdl.handle.net/10822/1050758
[15]: https://web.archive.org/web/20220922170031/https://www.nytimes.com/2012/03/01/technology/impatient-web-users-flee-slow-loading-sites.html
[16]: https://www.semanticscholar.org/paper/Tracking-Knowledge-Propagation-Across-Wikipedia-Valentim-Comarela/a3907f55ab5e5853351529db8e03e5784a93a368
[17]: https://doi.org/10.18653/v1/2020.acl-main.504
[18]: https://arxiv.org/abs/2201.05767
[19]: https://aclanthology.org/2021.eacl-main.261
[20]: https://arxiv.org/abs/2207.04993
[21]: https://doi.org/10.1007/978-3-030-45442-5_31
[22]: https://arxiv.org/abs/2110.07150
[23]: https://neuclir.github.io/
[24]: https://trec.nist.gov/
[25]: https://github.com/allenai/smashed
[26]: https://pytorch.org/data/beta/index.html
[27]: https://huggingface.co/docs/datasets/
[28]: https://springs.soldaini.net/
[29]: https://github.com/soldni/trouting
[30]: https://github.com/Georgetown-IR-Lab/QuickUMLS
[31]: http://dx.doi.org/10.18653/v1/2021.findings-acl.374
[32]: https://doi.org/10.1145/3366423.3380064
[33]: https://people.cs.georgetown.edu/~nazli/
[34]: https://ir.cs.georgetown.edu
[35]: https://allenai.org/olmo
[36]: https://allenai.org/blog/olmo2
[37]: https://allenai.org/blog/olmoe-an-open-small-and-state-of-the-art-mixture-of-experts-model-c258432d0514
[38]: https://allenai.org/blog/olmo-open-language-model-87ccfc95f580
[39]: https://allenai.org/blog/tulu-3
[40]: https://allenai.org/blog/tulu-3-405B
[41]: https://molmo.allenai.org/blog
[42]: https://allenai.org/blog/olmo2-32B
[43]: https://kyleclo.com/
[44]: /cats1.webp
[45]: /cats2.webp
[46]: https://allenai.org/blog/olmo3
[47]: https://allenai.org/blog/dr-tulu
[48]: https://x.com/AkariAsai/status/1993336112237117819


<!-- <li>
        <span class="list-icon icon-s2" aria-hidden="true"></span>
        <a href="https://www.semanticscholar.org/author/Luca-Soldaini/3328733" target="_blank">Semantic Scholar</a>
</li> -->

<!--<li class="contact-item">
    <span class="list-icon icon-bluesky" aria-hidden="true"></span>
    <a href="https://bsky.app/profile/soldaini.net">Bluesky</a>
    <span class="username-link" aria-hidden="true">@soldaini.net</span>
</li>-->
