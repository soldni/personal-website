+++
title = "Luca Soldaini"
+++

<div id="avatar-container">
    <div id="front-avatar">
        <img src="personal-me/me-512.webp" alt="Luca Soldaini" title="Portrait of Luca; they have pink hair parted to one side, with undercut. They are wearing an hawaiian shirt." class="avatar">
    </div>
    <div id="back-avatar">
        <img src="/alt.webp" loading="lazy" alt="A raccoon wearing a top hat and holding a pizza slice. Luca sometimes uses this image as their online profile picture." title="DALL‚Ä¢E (April 2022) generated image for the following prompt: 'oil painting of a raccoon with a tophat and monocle with a slice of fancy pizza.' Luca uses this image for theirs work account" class="avatar">
        <p class="tiny-text center caption-avatar">DALL‚Ä¢E 2 generation (April 2022)</a></p>
    </div>
</div>

Hello, visitor! üëã

<div id='about-me'>

I am a **lead research scientist** at the [Allen Institute for AI][6] in the [OLMo][35] team.
Prior to joining Ai2, I was a senior applied scientist at [Amazon Alexa][1].
I completed my Ph.D. in computer science at [Georgetown University][4] in 2018 in the [Information Retrieval Lab][34] working with [Nazli Goharian][33].

When not in front of a screen, I enjoy [brewing espresso][9], going on [runs][8], dreaming about utopian [mass transit systems][10], and curating my ever-growing [laptop stickers collection][11].
Raccoons are [the best][13].

</div>
<div id='research-summary'>

## Research Interests

These days, my research focuses on maximizing transparency in all aspects of how large language models (LLMs) are created, trained, and evaluated.

- üèéÔ∏è I co-lead the **data team for [OLMo][35]**, Ai2's language model. OLMo is a state-of-the-art, fully-open model designed to accelerate the science of LLMs. In 2024, we released [dense][38] and [mixture-of-experts][37] variants, alongside data, code, recipes, and checkpoints we made to create them. The OLMo project has been recognized with [**two best paper awards**](https://2024.aclweb.org/program/best_papers) at ACL 2024. We recently published [OLMo 2 7B and 13B][36], the best fully-open models yet.
- ‚öôÔ∏è With my colleagues at Ai2, I develop **recipes for adaptation** of language models. In 2024, we launched [T√ºlu 3][39], a state-of-the-art pipeline to post-train language models [up to 405B parameters][40]. We also launched [Molmo][41], a family of open state-of-the-art multimodal AI models.
- üß¨ I collaborated on several projects to **analyze and improve pipelines** for language models. [AboutMe](https://arxiv.org/abs/2401.06408), [WIMBD](https://arxiv.org/abs/2310.20707), and [WebOrganizer](https://arxiv.org/abs/2502.10341) are tools to analyze large pretraining corpora. [olmOCR](https://olmocr.allenai.org) is an high-performance toolkit for PDF text extraction. We also developed [predictive techniques](https://arxiv.org/abs/2412.04403) and [benchmarks](https://arxiv.org/abs/2312.10523) to characterize the behavior of language models during pretraining.

Beside core language modeling research, I am interested in adapting language models to information retrieval and document understanding tasks.

- üîé I have been investigating how to improve **interface between language models and retrieval systems**. With [Orion Weller](https://orionweller.github.io), we [studied](https://arxiv.org/abs/2309.08541) when generative models can be used to augment queries and documents in IR systems, and proposed  [FollowIR](https://arxiv.org/abs/2403.15246), a technique to adapt neural IR models to work with instructions. FollowIR was [extended](https://arxiv.org/abs/2501.19264) to multilingual systems.
- üìö Adapting LLMs to **literature-grounded scientific tasks** remains challenging, from [document parsing](https://aclanthology.org/2023.emnlp-demo.45/), to [instruction](https://arxiv.org/abs/2403.03866) [following](https://arxiv.org/abs/2406.07835), and [interface design](https://dl.acm.org/doi/10.1145/3665648). In late 2024, I collaborated on [*OpenSciLLM*](https://openscilm.allen.ai), an end-to-end demo showing how language models can be used for literature synthesis.

Hop over the [publications page](/publications) for a complete list of my work.
</div>

<div id='contacts'>

## Contacts

I <span aria-label="love">‚ù§</span> collaborating and connecting with other researchers!
Do **get in touch** if you are working in any of the areas above, or if you have ideas that you think I might be interested in.
<div id="contact-list">
<ul class="fa-ul">
    <li class="contact-item">
        <span class="list-icon icon-twitter" aria-hidden="true"></span>
        <a href="https://twitter.com/soldni">Twitter</a>
        <span class="username-link" aria-hidden="true">@soldni</span>
    </li>
    <li class="contact-item">
        <span class="list-icon icon-bluesky" aria-hidden="true"></span>
        <a href="https://bsky.app/profile/soldaini.net">Bluesky</a>
        <span class="username-link" aria-hidden="true">@soldaini.net</span>
    </li>
    <li class="contact-item">
        <span class="list-icon icon-github" aria-hidden="true"></span>
        <a href="https://github.com/soldni" target="_blank">GitHub</a>
        <span class="username-link" aria-hidden="true">@soldni</span>
    </li>
</ul>
<ul class="fa-ul">
    <li class="contact-item">
        <span class="list-icon icon-gs" aria-hidden="true"></span>
        <a href="https://scholar.google.com/citations?user=3KPvwcgAAAAJ" target="_blank">Google Scholar</a>
        <span class="username-link" aria-hidden="true"></span>
    </li>
    <li class="contact-item">
        <span class="list-icon icon-linkedin" aria-hidden="true"></span>
        <a href="https://www.linkedin.com/in/soldni" target="_blank">LinkedIn</a>
        <span class="username-link" aria-hidden="true"></span>
    </li>
    <li class="contact-item">
        <span class="list-icon icon-email" aria-hidden="true"></span>
        <a href="mailto:luca@soldaini.net">Email</a>
        <span class="username-link" aria-hidden="true"></span>
    </li>
</ul>
</div>
</div>

[1]: https://www.amazon.science/search?q=Luca+Soldaini&type=91d74bfc-4a20-30f0-8926-e52f02f15c04&type=5be10472-b2e0-37b5-b6f8-8f381832e94f&type=4f8e492c-6f2f-390e-bc61-f176d3a37ab9&s=0&expandedFilters=Type%2CResearch%2520area%2CTag%2CConference%2CJournal%2CAuthor%2CDate%2C
[2]: https://www.google.com/maps/place/Manhattan+Beach,+CA+90266/
[3]: https://www.ing-inl.unifi.it
[4]: https://cs.georgetown.edu/
[5]: http://queerinai.org/
[6]: https://allenai.org
[7]: https://research.semanticscholar.org
[8]: https://twitter.com/soldni/status/708678097483276289
[9]: https://twitter.com/soldni/status/1541146251537698816
[10]: /transit.webp
[11]: /laptop.webp
[12]: https://twitter.com/soldni/status/1444411540480749569
[13]: https://twitter.com/soldni/status/1437451814249517056
[14]: http://hdl.handle.net/10822/1050758
[15]: https://web.archive.org/web/20220922170031/https://www.nytimes.com/2012/03/01/technology/impatient-web-users-flee-slow-loading-sites.html
[16]: https://www.semanticscholar.org/paper/Tracking-Knowledge-Propagation-Across-Wikipedia-Valentim-Comarela/a3907f55ab5e5853351529db8e03e5784a93a368
[17]: https://doi.org/10.18653/v1/2020.acl-main.504
[18]: https://arxiv.org/abs/2201.05767
[19]: https://aclanthology.org/2021.eacl-main.261
[20]: https://arxiv.org/abs/2207.04993
[21]: https://doi.org/10.1007/978-3-030-45442-5_31
[22]: https://arxiv.org/abs/2110.07150
[23]: https://neuclir.github.io/
[24]: https://trec.nist.gov/
[25]: https://github.com/allenai/smashed
[26]: https://pytorch.org/data/beta/index.html
[27]: https://huggingface.co/docs/datasets/
[28]: https://springs.soldaini.net/
[29]: https://github.com/soldni/trouting
[30]: https://github.com/Georgetown-IR-Lab/QuickUMLS
[31]: http://dx.doi.org/10.18653/v1/2021.findings-acl.374
[32]: https://doi.org/10.1145/3366423.3380064
[33]: https://people.cs.georgetown.edu/~nazli/
[34]: https://ir.cs.georgetown.edu
[35]: https://allenai.org/olmo
[36]: https://allenai.org/blog/olmo2
[37]: https://allenai.org/blog/olmoe-an-open-small-and-state-of-the-art-mixture-of-experts-model-c258432d0514
[38]: https://allenai.org/blog/olmo-open-language-model-87ccfc95f580
[39]: https://allenai.org/blog/tulu-3
[40]: https://allenai.org/blog/tulu-3-405B
[41]: https://molmo.allenai.org/blog


<!-- <li>
        <span class="list-icon icon-s2" aria-hidden="true"></span>
        <a href="https://www.semanticscholar.org/author/Luca-Soldaini/3328733" target="_blank">Semantic Scholar</a>
</li> -->
